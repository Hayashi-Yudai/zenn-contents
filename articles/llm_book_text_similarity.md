---
title: "VRAMãŒå°‘ãªã„ç’°å¢ƒã§LLMã‚’åŠ¹ç‡çš„ã«fine-tuneã—ã¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿç¾ã™ã‚‹"
emoji: "ğŸ¤–"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["LLM", "ANN", "Python", "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"]
published: false
---

LLMå‘¨ã‚Šã®åŸºæœ¬çš„ãªçŸ¥è­˜ã¨Transformersã‚’ã‚‚ã£ã¨æ‰±ãˆã‚‹ã‚ˆã†ã«ãªã‚ŠãŸãã¦ã€æœ€è¿‘ [å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€](https://www.amazon.co.jp/dp/B0C9P7K6VH)ã‚’èª­ã‚“ã§ã„ãŸã®ã§ã™ãŒã€ãã®ä¸­ã§ã€Œãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è‰¯ã„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã¨ã„ã†ç¯€ãŒé¢ç™½ãã¦è‰²ã€…è‡ªåˆ†ã§è©¦ã—ã¦ã¿ã¦ã„ãŸã‚Šã—ã¾ã—ãŸã€‚ã“ã“ã§ã¯ã€è‡ªåˆ†ã®æ‰‹å…ƒã§æ–‡ç« ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã—ã¦ä½œã£ã¦è¦‹ãŸã®ã§ã€ãã‚Œã«ã¤ã„ã¦æ›¸ããŸã„ã¨æ€ã„ã¾ã™ã€‚

## å®Ÿé¨“ç’°å¢ƒ

- Ubuntu 20.04
- NVIDIA RTX2080 (VRAM: 8GB)
- Python 3.11

## å®Ÿé¨“

æ–‡ç« ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹ãŸã‚ã«ã€JGLUEã®JSTSã¨ã„ã†ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯Hugging Faceä¸Šã‹ã‚‰å–å¾—ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚«ãƒ©ãƒ ã‚’æŒã£ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚

https://huggingface.co/datasets/llm-book/JGLUE

- `sentence1`: 1ã¤ã‚ã®æ–‡ç« 
- `sentence2`: 2ã¤ã‚ã®æ–‡ç« 
- `label`: æ–‡ç« é–“ã®é¡ä¼¼åº¦(0 ~ 5)

1ã¤1ã¤ã®æ–‡ç« ã®é•·ã•ã¯çŸ­ã‚ã§ã€é•·ãã¦ã‚‚80æ–‡å­—ç¨‹åº¦ã«ãªã£ã¦ã„ã¾ã™ã€‚ãã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã®ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯LINEãƒ¤ãƒ•ãƒ¼ãŒå‡ºã—ã¦ã„ã‚‹ `line-distilbert-base-japanese` ã¨ã„ã†ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚

### ãƒ™ãƒ¼ã‚¹å®Ÿè£…

```python
class FineTuneModel(nn.Module):
    def __init__(self, base_model_name: str):
        super().__init__()

        self.text_encoder = AutoModel.from_pretrained(base_model_name)
        self.loss_fn = nn.MSELoss()

        self.global_step = 0

    def get_normlaized_embedding(self, sentence: BatchEncoding) -> Tensor:
        embedding = self.text_encoder(**sentence).last_hidden_state.mean(dim=1)
        return embedding / torch.norm(embedding, p=2, dim=-1, keepdim=True)

    def forward(
        self, sentence1: BatchEncoding, sentence2: BatchEncoding, similarities: Tensor
    ) -> ModelOutput:
        sentence1_embedding = self.get_normlaized_embedding(sentence1)
        sentence2_embedding = self.get_normlaized_embedding(sentence2)
        predicted_similarities = (sentence1_embedding * sentence2_embedding).sum(dim=1)
        loss = self.loss_fn(predicted_similarities, similarities)

        if self.training:
            self.global_step += 1

        return ModelOutput(loss=loss, logits=predicted_similarities)
```

é¡ä¼¼åº¦ã‚’å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã« `FineTuneModel` ã¨ã„ã†ãƒ¢ãƒ‡ãƒ«ã§ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ©ãƒƒãƒ—ã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ è‡ªä½“ã¯ã¨ã¦ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã§ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§embeddingã‚’å–å¾—ã—ãŸå¾Œã«ãƒãƒ¼ãƒãƒ©ã‚¤ã‚ºã—ã¦ã„ã‚‹ã ã‘ã§ã™ã€‚å­¦ç¿’æ™‚ã®æå¤±é–¢æ•°ã¯ã€2ã¤ã®embeddingã©ã†ã—ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ä¸ãˆã‚‰ã‚Œã¦ã„ã‚‹é¡ä¼¼åº¦ã®é–“ã®äºŒä¹—èª¤å·®ã«ãªã£ã¦ã„ã¾ã™ã€‚

### å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«

transformersã®ãƒ¢ãƒ‡ãƒ«ã‚’å˜ç´”ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã™ã‚‹å ´åˆã«ã¯ `TrainingArguments` ã®ä¸­ã§ `gradient_checkpointings=True` ã¨æŒ‡å®šã—ã¦ã‚„ã‚Œã°ã„ã„ã ã‘ãªã®ã§ã™ãŒã€ä»Šå›ã¯å¤‰ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ©ãƒƒãƒ—ã—ã¦ã—ã¾ã£ã¦ã„ã‚‹ã›ã„ã§ãã‚ŒãŒã§ãã¾ã›ã‚“ã€‚ã—ã‹ã—Transformersã® `PretrainedModel` å†…ã®å®Ÿè£…ã‚’è¦‹ã¦ã¿ã‚‹ã¨ `gradient_checkpointing_enable()` ã‚’ä½¿ã£ã¦ON/OFFã‚’åˆ‡ã‚Šæ›¿ãˆã¦ã„ã‚‹ã ã‘ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã®ã§ã€æ˜ç¤ºçš„ã«ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã ã‘ã§å®Ÿç¾ã§ãã¾ã—ãŸã€‚

https://github.com/huggingface/transformers/blob/7ee995fd9c692761c4601ddbffa2ac2ec9f27b0b/src/transformers/modeling_utils.py#L1161-L1165

ãªã®ã§ãƒ™ãƒ¼ã‚¹å®Ÿè£…ã¨ã®å¤‰æ›´ç‚¹ã¯ã»ã©ã‚“ã©ãªãä»¥ä¸‹ã®ã‚ˆã†ã«å®Ÿè£…ãŒå¯èƒ½ã§ã™ã€‚

```python
class FineTuneModel(nn.Module):
    def __init__(self, base_model_name: str):
        super().__init__()

        self.text_encoder = AutoModel.from_pretrained(base_model_name)
        self.text_encoder.gradient_checkpointing_enable()  # <- è¿½åŠ 
        self.loss_fn = nn.MSELoss()

        self.global_step = 0

    # ã“ã“ã‹ã‚‰ä¸‹ã¯åŒã˜
```

### å­¦ç¿’å®Ÿè¡Œç”¨ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```python
def collelate_fn(
    examples: list[dict[str, str | float]], tokenizer: AutoTokenizer
) -> dict[str, Any]:
    sentence1: str = []
    sentence2: str = []
    similarities: list[float] = []

    for example in examples:
        sentence1.append(example["sentence1"])
        sentence2.append(example["sentence2"])
        similarities.append(example["label"])

    tokenized_sentence1 = tokenizer(
        sentence1, max_length=512, return_tensors="pt", padding=True, truncation=True
    )
    tokenized_sentence2 = tokenizer(
        sentence2, max_length=512, return_tensors="pt", padding=True, truncation=True
    )

    del tokenized_sentence1["token_type_ids"]
    del tokenized_sentence2["token_type_ids"]

    return {
        "sentence1": tokenized_sentence1,
        "sentence2": tokenized_sentence2,
        "similarities": torch.tensor(similarities) / 2.5 - 1,
    }


if __name__ == "__main__":
    model_name = "line-corporation/line-distilbert-base-japanese"
    train_dataset = load_dataset("llm-book/JGLUE", name="JSTS", split="train")
    valid_dataset = load_dataset("llm-book/JGLUE", name="JSTS", split="validation")
    model = FineTuneModel(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    batch_size = 128
    output_dir = f"./output_jsts_baseline_{batch_size}"
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_eval_batch_size=batch_size,
        per_device_train_batch_size=batch_size,
        learning_rate=2e-5,
        num_train_epochs=30,
        lr_scheduler_type="linear",
        warmup_ratio=0.1,
        logging_strategy="epoch",
        save_strategy="epoch",
        evaluation_strategy="epoch",
        save_total_limit=1,
        fp16=True,
        remove_unused_columns=True,
        load_best_model_at_end=True,
        label_names=["similarities"],
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=lambda x: collelate_fn(x, tokenizer),
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],
    )
    trainer.train()

    model.text_encoder.save_pretrained(output_dir)

```

### å®Ÿé¨“çµæœ

ã“ã®2ã¤ã®å®Ÿè£…ã‚’æ¯”è¼ƒã—ã¦ã¿ã¾ã™ã€‚å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ã¯è¨“ç·´çµæœã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã‚‚ã®ã§ã¯ãªãã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æŠ‘ãˆã‚‹ä»£ã‚ã‚Šã«è¨ˆç®—ã‚¹ãƒ”ãƒ¼ãƒ‰ãŒå°‘ã—è½ã¡ã‚‹ã¨ã„ã£ãŸé¡ã®ã‚‚ã®ã§ã™ã€‚ãªã®ã§æ¯”è¼ƒå¯¾è±¡ã¨ã—ã¦ã¯

- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
- è¨ˆç®—ã‚¹ãƒ”ãƒ¼ãƒ‰

ã®2ç‚¹ã‚’è¦‹ã¾ã™ã€‚çµæœã‚’ã¾ã¨ã‚ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ« | batch size | è¨ˆç®—é€Ÿåº¦(iteration / sec) | VRAMä½¿ç”¨é‡
-- | -- | -- | --
baseline | 128 | 4.3 | 7.8 GB
baseline | 256 | OOM | >8.0 GB
å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚° | 128 | 4.0 | 3.6 GB
å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚° | 512 | 1.0 | 5.8 GB

è¨ˆç®—é€Ÿåº¦ã¯ç¢ºã‹ã« 4.3 â†’ 4.0ã¸ã¨7 %ã»ã©ä½ä¸‹ã—ã¦ã„ã¾ã™ãŒå¤§ã—ãŸå¤‰åŒ–ã§ã¯ãªã„ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚ã—ã‹ã—VRAMã®ä½¿ç”¨é‡ã¯åŠåˆ†ä»¥ä¸‹ã¾ã§æ¿€æ¸›ã—ã¦ãŠã‚Šã€batch sizeã‚’4å€ã®512ã¾ã§ä¸Šã’ã¦ã‚‚OOMã¯ã—ã¾ã›ã‚“ã§ã—ãŸã€‚

## é¡ä¼¼æ–‡æ›¸ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢

æ–‡ç« ã®é¡ä¼¼åº¦ã‚’å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½œã£ãŸã®ã§ã€ã“ã‚Œã‚’ä½¿ã£ã¦é¡ä¼¼æ–‡æ›¸ã®æ¤œç´¢ã‚’è©¦ã—ã¦ã¿ã¾ã™ã€‚ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’è¡Œã†ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯æ§˜ã€…ã‚ã‚Šã¾ã™ãŒã€ã“ã“ã§ã¯æœ€è¿‘SpotifyãŒOSSã¨ã—ã¦å…¬é–‹ã—ãŸVoyagerã‚’è©¦ã—ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚

https://spotify.github.io/voyager/

```python
import pickle

from datasets import load_dataset
import numpy as np
from transformers import AutoModel, AutoTokenizer
import torch
from tqdm import tqdm
from voyager import Index, Space


def calc_embedding(model, tokenizer, texts) -> np.ndarray:
    input = tokenizer(
        texts, return_tensors="pt", padding=True, truncation=True, max_length=512
    )
    del input["token_type_ids"]

    with torch.no_grad():
        output = model(**input)

    embedding = output.last_hidden_state.mean(dim=1)

    return embedding[0].numpy()


if __name__ == "__main__":
    model = AutoModel.from_pretrained(
        "./output_jsts_grad_ckpt_512", local_files_only=True
    )
    tokenizer = AutoTokenizer.from_pretrained(
        "line-corporation/line-distilbert-base-japanese"
    )

    create_index = True
    if create_index:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹å ´åˆ"""
        valid_dataset = load_dataset("llm-book/JGLUE", name="JSTS", split="validation")
        texts = []
        for example in valid_dataset:
            texts.append(example["sentence1"])
            texts.append(example["sentence2"])

        texts = list(set(texts))
        with open("./texts_list", "wb") as f:
            pickle.dump(texts, f)

        index = Index(Space.Cosine, num_dimensions=768)
        batch_size = 128
        for i in tqdm(range(0, len(texts) // batch_size + 1)):
            embeddings = texts[i * batch_size : (i + 1) * batch_size]
            for embedding in embeddings:
                _ = index.add_item(calc_embedding(model, tokenizer, embedding))

        index.save("index.voy")
    else:
        """ä½œæˆæ¸ˆã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ã†å ´åˆ"""
        index = Index.load("index.voy")
        texts = pickle.load(open("./texts_list", "rb"))

    neighbors, distances = index.query(
        calc_embedding(model, tokenizer, "ã‚¸ãƒ£ãƒ³ãƒ—å°ã‹ã‚‰ç”·æ€§ã‚¹ã‚±ãƒ¼ãƒˆãƒœãƒ¼ãƒ€ãƒ¼ãŒã‚¸ãƒ£ãƒ³ãƒ—ã—ã¦ã„ã¾ã™ã€‚"), k=2
    )

    print(texts[neighbors[0]])
    print(texts[neighbors[1]])

```

ä¸Šã®ã‚³ãƒ¼ãƒ‰ã§è©¦ã—ã¦ã„ã‚‹ä¾‹ã¯validationãƒ‡ãƒ¼ã‚¿å†…ã«ã‚ã‚‹æ–‡ç« ã§ã™ã€‚ã“ã‚Œã«è¿‘ã„æ–‡ç« ã‚’å¼•ã£å¼µã£ã¦ããŸçµæœã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

```bash
ã‚¸ãƒ£ãƒ³ãƒ—å°ã‹ã‚‰ç”·æ€§ã‚¹ã‚±ãƒ¼ãƒˆãƒœãƒ¼ãƒ€ãƒ¼ãŒã‚¸ãƒ£ãƒ³ãƒ—ã—ã¦ã„ã¾ã™ã€‚
ã‚¹ã‚±ãƒ¼ãƒˆãƒœãƒ¼ãƒ€ãƒ¼ãŒã‚¸ãƒ£ãƒ³ãƒ—å°ã§ã‚¸ãƒ£ãƒ³ãƒ—ã—ã¦ã„ã¾ã™ã€‚
```

ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†…ã«å­˜åœ¨ã™ã‚‹æ–‡ç« ã§æ¤œç´¢ã—ãŸã®ã§ã‚‚ã¡ã‚ã‚“topã¯ã‚¯ã‚¨ãƒªã®æ–‡æ›¸ãã‚Œè‡ªä½“ãŒè¿”ã£ã¦ãã¾ã™ã€‚2ç•ªç›®ã«è¿‘ã„æ–‡ç« ã‚‚ã‚¹ã‚±ãƒ¼ãƒˆã‚„ã‚¸ãƒ£ãƒ³ãƒ—ã¨ã„ã£ãŸå…±é€šç‚¹ãŒã‚ã‚‹ã®ã§è¿‘ã„æ–‡ç« ã¨è¨€ãˆãã†ã§ã™ã€‚æ¬¡ã«ã€è‡ªåˆ†ã§è€ƒãˆãŸæ–‡ç« ã‚’é©å½“ã«å…¥ã‚Œã¦ã¿ã¾ã™ã€‚ã€Œä»Šæ—¥ã¯ãšã£ã¨å®¶ã§æœ¬ã‚’èª­ã‚“ã§ã„ã¾ã—ãŸã€‚ã€ã¨ã„ã†æ–‡ç« ã‚’å…¥ã‚ŒãŸçµæœãŒä»¥ä¸‹ã§ã™ã€‚

```bash
ã‚­ãƒƒãƒãƒ³ã§ä½•ã‚‚ã›ãšã«è©±ã—ã¦ã„ã‚‹
ãƒªãƒ“ãƒ³ã‚°ã§é£²ã¿ã‚‚ã®ã‚’é£²ã‚€äººãŸã¡
```

ã€Œãšã£ã¨å®¶ã§ã€ã¨ã„ã†éƒ¨åˆ†ãŒã®ã‚“ã³ã‚Šã—ã¦ã„ã‚‹ã‚ˆã†ãªãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚’ä¸ãˆã‚‹ã®ã‹ã€å®¶ã®ä¸­ã§ã®ã‚“ã³ã‚Šã—ã¦ã„ã‚‹æ„Ÿã˜ã®æ–‡ç« ãŒè¿‘ã„ã¨åˆ¤å®šã•ã‚Œã¦ã„ã¾ã™ã€‚

## æ„Ÿæƒ³

Transformersã¯ä»Šã¾ã§è»½ã„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã«ä½¿ã£ãŸã“ã¨ã¯ã‚ã£ãŸã®ã§ã™ãŒã€è‰²ã€…å·¥å¤«ã—ã¦ã¿ã‚‹ã¨ã„ã£ãŸéƒ¨åˆ†ã¯ã—ãŸã“ã¨ãŒãªã‹ã£ãŸã®ã§ã¨ã¦ã‚‚å‹‰å¼·ã«ãªã‚Šã¾ã—ãŸã€‚æœ¬å½“ã¯LoRAã‚’è©¦ã™ã¨ã„ã†ã“ã¨ã‚‚ã‚„ã£ã¦ã¿ãŸã‹ã£ãŸã®ã§ã™ãŒã€ä»Šå›ã¯ã†ã¾ãå‹•ã‹ã›ãšæ–­å¿µã—ã¾ã—ãŸã€‚ä»Šåº¦å†ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã—ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚

æœ€å¾Œã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®éƒ¨åˆ†ã¯æœ¬ã®å†…å®¹ã‹ã‚‰ã¯å°‘ã—é›¢ã‚Œã¦ã€è‡ªåˆ†ãŒæœ€è¿‘æ°—ã«ãªã£ã¦ã„ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’è©¦ã™ã¨ã„ã†ã“ã¨ã‚’ã‚„ã£ã¦ã¿ã¾ã—ãŸã€‚Voyagerã¯ã¨ã¦ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã—ã£ã‹ã‚Šã—ã¦ã„ã‚‹ã¨ã„ã†ã“ã¨ã¯èã„ã¦ã„ã¦çŸ¥ã£ã¦ã¯ã„ãŸã®ã§ã™ãŒã€è‡ªåˆ†ã§å®Ÿéš›ã«è©¦ã—ã¦ã¿ã¦ã‚„ã‚ŠãŸã„ã“ã¨ã‚’ã™ãã«å®Ÿç¾ã§ããŸã®ã§ä½“é¨“ã¯ã‚‚ã®ã™ã”ãè‰¯ã‹ã£ãŸã§ã™ã€‚Voyagerã®æ€§èƒ½ã«é–¢ã—ã¦ã¯ç¢ºèªã—ãŸã“ã¨ãŒãªã‹ã£ãŸã®ã§ä»Šåº¦è©¦ã—ã¦ã¿ãŸã„ã§ã™ã€‚